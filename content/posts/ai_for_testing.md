---
title: "大厂的测试人员在用ai干点啥"
date: 2024-11-15T07:11:01+08:00
draft: false
---

上周末参加了可能是国内顶级的 AI 技术峰会，主要观摩了测试专场的内容，全程基本都是大厂和高校进行分享，应该可以部分代码国内的头部水平，感触良多，跟大家分享一下。

可以看到大模型出现以来，很多测试同学都在探索 ai 在测试领域的应用，这一年多以来也有不少的落地项目和产出，总的来说还是让人颇受启发的。

简单回顾一下，大家的发力点大概在下面几个方面。

## 接口测试

ai 在接口测试中的应用大概有下面两种。

- 通过大模型和 RAG 结合，使用模型的推理能力，让模型根据 api 文档自动生成用例和断言
- 使用大语言模型生成接口测试的数据

讨论这个议题的同学很多，大家的实践也都比较深入了，特别是在跟现有系统的整合方面，有些公司可以做到在内部测试平台上直接一键生成各种接口的测试用例，并运行生产报告，在工程方面的探索还是值得称道的。

不过当前的进展也有很多不完善的地方

- 大家都没有很好的指标可以统计大模型生成用例的可用性以及准确性
- 没有人真正的展示 ai 生成的断言是什么样子的，在分享后的交流里，有的讲师表示目前他们只实现了非常简单的不带业务逻辑的断言，比如响应状态码的断言

## UI 自动化测试

似乎只有 1 位大厂的讲师分享了他们在 UI 自动化领域的探索，他们的当前的进展可能是

- 通过 Agent 自动生成 ui 自动化用例，具体的实现细节也是先简化 dom，然后使用 agent 进行推断
- 通过 Agent 实现 ui 自动化用例自动更新的能力，比如更新页面上发生变化的元素

一些我觉得不是很清楚的地方是

- 项目似乎是进行时，并没有真正落地并大规模使用
- ui 自动化用例的更新范围似乎只是页面上发生变化的元素，如果系统流程发生了些许变化，大模型可能也是爱莫能助的

## 模型效果评估

使用指标对大模型的效果进行评估也是测试人员工作的一部分。

这里测试人员需要

- 准备测试数据集，也可以用 ai 生成测试数据集
- 进行自动化验证并提炼模型效果指标

这一块我不是很懂，不过目前看来大家用的可能是业内统一的评测框架和指标，这块的天花板很高，有很大的深入研究的空间。

## 需求分类

使用 Agent 和 RAG 对需求进行分类和打分，标识出需求风险的级别，高风险的需求需要进行人工测试，低风险的需求可以免测。

这是整场分享最吸引我的议题。

因为这个想法直接抓住了测试的核心问题，那就是需求问题。

目前看来这个方案的召回率很高，准确性还有提升的空间。

这个项目的局限性是

- 模型对多模态的文档没有很好的处理办法，毕竟很多需求里图文并茂，不能识别图片和视频实在是比较遗憾

## 总结

总的看来很多大厂都在大模型与测试相结合方面积极的进行实践和落地，大家在工程化方面的探索还是非常积极主动的。

不过目前看杀手级应用还是没有，最近正好 openai 和 claude ai 都在智能体上面重点发力。

智能体的一个核心功能就是像人类一样操作电脑和其他应用，如果真能落地的话可能对自动化测试带来不小的益处。
